diff --git a/ding/entry/serial_entry_pc.py b/ding/entry/serial_entry_pc.py
index d173cea2..0b74f67f 100644
--- a/ding/entry/serial_entry_pc.py
+++ b/ding/entry/serial_entry_pc.py
@@ -42,6 +42,64 @@ def get_vi_sequence(env, observation):
 
     vi_map = np.full((env.size, env.size), fill_value=env.n_action, dtype=np.int32)
 
+    found_start = False
+    while current_points and not found_start:
+        next_points = []
+        for point_x, point_y in current_points:
+            for (action, (next_point_x, next_point_y)) in [(0, (point_x - 1, point_y)), (1, (point_x, point_y - 1)),
+                                                           (2, (point_x + 1, point_y)), (3, (point_x, point_y + 1))]:
+
+                if (next_point_x, next_point_y) in visited_points:
+                    continue
+
+                if not (0 <= next_point_x < len(nav_map) and 0 <= next_point_y < len(nav_map[next_point_x])):
+                    continue
+
+                if nav_map[next_point_x][next_point_y] == 'x':
+                    continue
+
+                next_points.append((next_point_x, next_point_y))
+                visited_points[(next_point_x, next_point_y)] = True
+                chosen_actions[(next_point_x, next_point_y)] = action
+                vi_map[next_point_x, next_point_y] = action
+
+                if next_point_x == start_x and next_point_y == start_y:
+                    found_start = True
+        vi_sequence.append(vi_map.copy())
+        current_points = next_points
+    track_back = []
+    if found_start:
+        cur_x, cur_y = start_x, start_y
+        while cur_x != target_location[0] or cur_y != target_location[1]:
+            act = vi_sequence[-1][cur_x, cur_y]
+            track_back.append((
+                torch.FloatTensor(env.process_states([cur_x, cur_y], env.get_maze_map())),
+                act))
+            if act == 0:
+                cur_x += 1
+            elif act == 1:
+                cur_y += 1
+            elif act == 2:
+                cur_x -= 1
+            elif act == 3:
+                cur_y -= 1
+
+    return np.array(vi_sequence), track_back
+
+
+def get_vi_sequence_bak(env, observation):
+    """Returns [L, W, W] optimal actions."""
+    xy = np.where(observation[Ellipsis, -1] == 1)
+    start_x, start_y = xy[0][0], xy[1][0]
+    target_location = env.target_location
+    nav_map = env.nav_map
+    current_points = [target_location]
+    chosen_actions = {target_location: 0}
+    visited_points = {target_location: True}
+    vi_sequence = []
+
+    vi_map = np.full((env.size, env.size), fill_value=env.n_action, dtype=np.int32)
+
     found_start = False
     while current_points and not found_start:
         next_points = []
@@ -109,16 +167,21 @@ def load_2d_datasets(train_seeds=5, test_seeds=1, batch_size=32):
             bfs_input_maps = bfs_input_maps_test
             bfs_output_maps = bfs_output_maps_test
 
-        env_observations = torch.stack([torch.from_numpy(env.random_start()) for _ in range(80)])
-        # assert False
-        # env_observations = torch.squeeze(env_steps.observation, axis=1)
+        # env_observations = torch.stack([torch.from_numpy(env.random_start()) for _ in range(80)])
+        start_obs = env.process_states(env._get_obs(), env.get_maze_map())
+        _, track_back = get_vi_sequence(env, start_obs)
+        env_observations = torch.stack([
+            track_back[i][0] for i in range(len(track_back))
+        ], dim=0)
+
         for i in range(env_observations.shape[0]):
-            bfs_sequence = get_vi_sequence(env, env_observations[i].numpy().astype(np.int32))  # [L, W, W]
+            bfs_sequence, _ = get_vi_sequence(env, env_observations[i].numpy().astype(np.int32))  # [L, W, W]
             bfs_input_map = env.n_action * np.ones([env.size, env.size], dtype=np.long)
-            for _ in range(50):
-                bfs_input_maps.append(torch.from_numpy(copy.deepcopy(bfs_input_map)))
-                bfs_output_maps.append(torch.from_numpy(copy.deepcopy(bfs_sequence[0])))
-                observations.append(copy.deepcopy(env_observations[i]))
+            # Repeat the first frame.
+            # for _ in range(50):
+            #     bfs_input_maps.append(torch.from_numpy(copy.deepcopy(bfs_input_map)))
+            #     bfs_output_maps.append(torch.from_numpy(copy.deepcopy(bfs_sequence[0])))
+            #     observations.append(copy.deepcopy(env_observations[i]))
 
             for j in range(bfs_sequence.shape[0]):
                 bfs_input_maps.append(torch.from_numpy(bfs_input_map))
@@ -195,21 +258,6 @@ def serial_pipeline_pc(
     stop = False
     iter_cnt = 0
     for epoch in range(cfg.policy.learn.train_epoch):
-        # Evaluate policy performance
-        # loss_list = []
-        # for _, bat in enumerate(eval_loader):
-        #     res = policy._forward_eval(bat['obs'])
-        #     if cont:
-        #         loss_list.append(torch.nn.L1Loss()(res['action'], bat['action'].squeeze(-1)).item())
-        #     else:
-        #         res = torch.argmax(res['logit'], dim=1)
-        #         loss_list.append(torch.sum(res == bat['action'].squeeze(-1)).item() / bat['action'].shape[0])
-        # if cont:
-        #     label = 'validation_loss'
-        # else:
-        #     label = 'validation_acc'
-        # tb_logger.add_scalar(label, sum(loss_list) / len(loss_list), iter_cnt)
-
         # train
         criterion = torch.nn.CrossEntropyLoss()
         for i, train_data in enumerate(dataloader):
@@ -229,7 +277,7 @@ def serial_pipeline_pc(
                                                             test_data['bfs_out'].long()
             states = observations
             bfs_input_onehot = torch.nn.functional.one_hot(bfs_input_maps, 5).float()
-            shape0, shape1 = bfs_input_maps.shape[1], bfs_input_maps.shape[2]
+            # shape0, shape1 = bfs_input_maps.shape[1], bfs_input_maps.shape[2]
             # is_init = torch.zeros([bfs_input_maps.shape[0], shape0, shape1, 2]).float().to(bfs_input_maps.device)
             # tmp = torch.sum(bfs_input_maps, dim=(1, 2))
             # tmp = (tmp == 4 * shape0 * shape1).long()
diff --git a/ding/model/template/pc.py b/ding/model/template/pc.py
index 0ea1fdb1..50e1436d 100644
--- a/ding/model/template/pc.py
+++ b/ding/model/template/pc.py
@@ -99,16 +99,47 @@ class PC(nn.Module):
             padding=padding_sizes,
         )
 
-        # if self._augment:
-        #     self._augment_layers = nn.Sequential([
-        #         tf.keras.layers.RandomCrop(maze_size, maze_size),
-        #         tf.keras.layers.RandomTranslation((-0.1, 0.1), (-0.1, 0.1),
-        #                                           fill_mode='constant'),
-        #         tf.keras.layers.RandomZoom((-0.1, 0.1), (-0.1, 0.1),
-        #                                    fill_mode='constant'),
-        #     ])
-
     def forward(self, x):
         x = x.permute(0, 3, 1, 2)
         x = self._encoder(x)
         return {'logit': x.permute(0, 2, 3, 1)}
+
+
+@MODEL_REGISTRY.register('pbc')
+class PBC(nn.Module):
+
+    def __init__(
+        self,
+        obs_shape: Union[int, SequenceType],
+        action_shape: Union[int, SequenceType],
+        encoder_hidden_size_list: SequenceType = [128, 128, 256, 256],
+        augment=False
+    ):
+        super().__init__()
+
+        self._augment = augment
+        num_layers = len(encoder_hidden_size_list)
+
+        kernel_sizes = (3, ) * (num_layers + 1)
+        stride_sizes = (1, ) * (num_layers + 1)
+        padding_sizes = (1, ) * (num_layers + 1)
+        encoder_hidden_size_list.append(action_shape + 1)
+
+        self._encoder = ConvEncoder(
+            obs_shape=obs_shape,
+            hidden_size_list=encoder_hidden_size_list,
+            kernel_size=kernel_sizes,
+            stride=stride_sizes,
+            padding=padding_sizes,
+        )
+        self.head = nn.Sequential(
+            nn.Flatten(),
+            nn.ReLU(),
+            nn.Linear(16*16*5, 4)
+        )
+
+    def forward(self, x):
+        x = x.permute(0, 3, 1, 2)
+        x = self._encoder(x)
+        x = self.head(x)
+        return {'logit': x}
diff --git a/dizoo/maze/envs/maze_env.py b/dizoo/maze/envs/maze_env.py
index a125505e..d1f3c05a 100644
--- a/dizoo/maze/envs/maze_env.py
+++ b/dizoo/maze/envs/maze_env.py
@@ -93,12 +93,15 @@ class Maze(gym.Env):
         )
 
     def random_start(self):
+        init_x, init_y = self._x, self._y
         while True:  # Find empty grid cell.
             self._x = self.np_random.integers(self._max_x)
             self._y = self.np_random.integers(self._max_y)
             if self._map[self._x][self._y] != 'x':
                 break
-        return self.process_states(self._get_obs(), self.get_maze_map())
+        ret = copy.deepcopy(self.process_states(self._get_obs(), self.get_maze_map()))
+        self._x, self._y = init_x, init_y
+        return ret
 
     def close(self) -> None:
         if self._init_flag:
@@ -350,6 +353,7 @@ class Maze(gym.Env):
             done = True
         if done:
             info['final_eval_reward'] = reward
+            info['eval_episode_return'] = reward
         return BaseEnvTimestep(self.process_states(self._get_obs(), self.get_maze_map()), reward, done, info)